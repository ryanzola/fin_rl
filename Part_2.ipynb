{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce94973f-9b6a-4a16-b90d-37fd1c057aaf",
   "metadata": {},
   "source": [
    "# FinRL\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e50e5397-2d69-439a-aa86-8294873ca849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f566f-db2d-492b-b191-e82195007871",
   "metadata": {},
   "source": [
    "### Load the Data from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de52139-c268-49e1-a36c-4187d1c8972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359d5c2-32c1-46a8-9d58-41b607f07e81",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e3dfe77-c6a8-45be-b7c9-402565ee1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 4, State Space: 41\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f'Stock Dimension: {stock_dimension}, State Space: {state_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f5ed66e-0d7d-4b26-9fe1-2e390d1f6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    'hmax': 100,\n",
    "    'initial_amount': 1000000,\n",
    "    'num_stock_shares': num_stock_shares,\n",
    "    'buy_cost_pct': buy_cost_list,\n",
    "    'sell_cost_pct': sell_cost_list,\n",
    "    'state_space': state_space,\n",
    "    'stock_dim': stock_dimension,\n",
    "    'tech_indicator_list': INDICATORS,\n",
    "    'action_space': stock_dimension,\n",
    "    'reward_scaling': 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f485c0b-4ec8-460a-afbc-bb17d98f8409",
   "metadata": {},
   "source": [
    "### Environment for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74f3faa3-5567-4db6-b43a-a8e353035253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df48b5-a866-4646-85d2-9e491321eef3",
   "metadata": {},
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "902cd0e5-e299-49a4-bc80-2a7f9ec14fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016faf63-1d0b-4c37-990f-68abea8a3039",
   "metadata": {},
   "source": [
    "### A2C Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f72fb5-31e8-4781-8524-065616ae3f93",
   "metadata": {},
   "source": [
    "The code above and below provide a healthy framework to add different agent types within the same notebook. For this I'll only be training sn A2C model, but may add ddpg, ppo, and td3 later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5486a9eb-bc6e-4bb7-88fb-2dce3ce99d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "model_a2c = agent.get_model('a2c')\n",
    "model_ppo = agent.get_model('ppo')\n",
    "\n",
    "if if_using_a2c:\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "\n",
    "# Come back to this later\n",
    "if if_using_ppo:\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # set new logger\n",
    "    model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45dc255b-96e6-4257-b936-6bf5b9491f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 929         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.75       |\n",
      "|    explained_variance | -1.53       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -3.65       |\n",
      "|    reward             | -0.47909597 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.502       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 960       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0.108     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.54     |\n",
      "|    reward             | 0.9521666 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.94      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 967      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.75    |\n",
      "|    explained_variance | 0.194    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -20.2    |\n",
      "|    reward             | 5.627131 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 13.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 977        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | -0.038     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 7.37       |\n",
      "|    reward             | -1.0165615 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 20.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 983        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.75      |\n",
      "|    explained_variance | 0.00842    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 77.1       |\n",
      "|    reward             | -17.072828 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 283        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 985          |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -5.76        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 1.78         |\n",
      "|    reward             | -0.059515126 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.136        |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 986      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.77    |\n",
      "|    explained_variance | -0.00561 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -6.06    |\n",
      "|    reward             | 2.891096 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 11.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 989        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.77      |\n",
      "|    explained_variance | -0.0203    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 2.68       |\n",
      "|    reward             | -1.5871831 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 991        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 9.79       |\n",
      "|    reward             | -1.9060695 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 988        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 99.9       |\n",
      "|    reward             | -17.081438 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 237        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 990       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.75     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -95.5     |\n",
      "|    reward             | 6.9376855 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 374       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 991         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.75       |\n",
      "|    explained_variance | -4.77e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 47.6        |\n",
      "|    reward             | -0.08474723 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 69.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 992        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.75      |\n",
      "|    explained_variance | -0.000333  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -4.37      |\n",
      "|    reward             | -15.138821 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 992       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.76     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 26.6      |\n",
      "|    reward             | 10.153217 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 42.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 993       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.75     |\n",
      "|    explained_variance | -0.00327  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 31        |\n",
      "|    reward             | 21.032118 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 27        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 994         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.76       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 91.3        |\n",
      "|    reward             | 0.017180504 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 250         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 994       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -33.9     |\n",
      "|    reward             | 24.899963 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 197       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 970        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -0.0928    |\n",
      "|    reward             | 0.23101297 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0141     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 970        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | -0.108     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -3.84      |\n",
      "|    reward             | 0.11430952 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.96       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 972         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.75       |\n",
      "|    explained_variance | 0.0377      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -38.8       |\n",
      "|    reward             | -0.92663324 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 48.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 973       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.75     |\n",
      "|    explained_variance | 0.0105    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 1.97      |\n",
      "|    reward             | 1.7329601 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 974        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -18.3      |\n",
      "|    reward             | -13.978497 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 37.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 974       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.75     |\n",
      "|    explained_variance | -0.000227 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -509      |\n",
      "|    reward             | 17.237062 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.1e+04   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 975        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 13.9       |\n",
      "|    reward             | 0.53723884 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 976       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0.0201    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 1.11      |\n",
      "|    reward             | 2.0948153 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 977       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.77     |\n",
      "|    explained_variance | -0.012    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -2.64     |\n",
      "|    reward             | 0.6678566 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 978       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.77     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 17.5      |\n",
      "|    reward             | 0.7280127 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 978       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -91.5     |\n",
      "|    reward             | 14.174231 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 243       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 979        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -1.92      |\n",
      "|    reward             | 0.09262485 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.125      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 980       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0.138     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 5.71      |\n",
      "|    reward             | 2.0866625 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.82      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 981          |\n",
      "|    iterations         | 3100         |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 15500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -5.74        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3099         |\n",
      "|    policy_loss        | -16.4        |\n",
      "|    reward             | 0.0054526036 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 8.16         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 982        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.73      |\n",
      "|    explained_variance | -0.00499   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 18.3       |\n",
      "|    reward             | 0.20564497 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 25.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 982       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.73     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 1.28      |\n",
      "|    reward             | -5.710664 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.94      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 2.62      |\n",
      "|    reward             | -9.289701 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.67      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 983        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.73      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 8.25       |\n",
      "|    reward             | 0.37413505 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 984        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.71      |\n",
      "|    explained_variance | 0.000107   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 77.2       |\n",
      "|    reward             | -5.1721535 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 205        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 985        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 60.5       |\n",
      "|    reward             | 0.22575109 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 128        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 985       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.72     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 25.7      |\n",
      "|    reward             | 5.808158  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 22.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 986      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 10.6     |\n",
      "|    reward             | 6.039552 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 11.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 987       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -33.5     |\n",
      "|    reward             | 1.4194003 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 60.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 987        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.73      |\n",
      "|    explained_variance | 0.0426     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 15.9       |\n",
      "|    reward             | 0.35285494 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 15         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 987        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.73      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | -3.6152697 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 440        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 987       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -30.9     |\n",
      "|    reward             | 1.2884765 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 42.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -16.4     |\n",
      "|    reward             | 1.7491121 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 14.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -28.9     |\n",
      "|    reward             | 0.3894983 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 35.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.73     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 1.45      |\n",
      "|    reward             | 3.2629197 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 17.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 989      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.71    |\n",
      "|    explained_variance | 0.0881   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -9.16    |\n",
      "|    reward             | 2.165374 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 5.44     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 989        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.71      |\n",
      "|    explained_variance | 0.0164     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -101       |\n",
      "|    reward             | -0.5459431 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 235        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 989       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.7      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 41.8      |\n",
      "|    reward             | 1.4258633 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 71.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 990       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 16.7      |\n",
      "|    reward             | 1.9465039 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 35.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 990        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.71      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 63.8       |\n",
      "|    reward             | -1.8335139 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 89.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 991       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -42.7     |\n",
      "|    reward             | 5.5219154 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 77.8      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4054488.14\n",
      "total_reward: 3054488.14\n",
      "total_cost: 3497.70\n",
      "total_trades: 9938\n",
      "Sharpe: 0.552\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 991       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 2.5       |\n",
      "|    reward             | 1.4376475 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.98      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 991        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -31.2      |\n",
      "|    reward             | -3.8635955 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 26.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 991       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | 0.00985   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -16.6     |\n",
      "|    reward             | 4.5006943 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 992        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.69      |\n",
      "|    explained_variance | -0.00399   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -5.55      |\n",
      "|    reward             | -1.5579327 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 982        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -196       |\n",
      "|    reward             | -22.565277 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.55e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 982        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.7       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 2.31       |\n",
      "|    reward             | 0.23740377 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.805      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 983        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.7       |\n",
      "|    explained_variance | 0.000384   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 42.3       |\n",
      "|    reward             | 0.05555159 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 68.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.71     |\n",
      "|    explained_variance | -0.00973  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 5.04      |\n",
      "|    reward             | 0.2308055 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 982       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | -0.00241  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | -2.930922 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | 0.00179   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 1.1972924 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 31.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.66     |\n",
      "|    explained_variance | 6.49e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 138       |\n",
      "|    reward             | 32.159332 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.05e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | -0.137    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 14.3      |\n",
      "|    reward             | 0.6852471 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 983       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 7.08      |\n",
      "|    reward             | -4.932884 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 30.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 984         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.69       |\n",
      "|    explained_variance | -1.04e-05   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -3.05       |\n",
      "|    reward             | -0.52988994 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 26.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 984        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -211       |\n",
      "|    reward             | -13.582744 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.44e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 984        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -29.6      |\n",
      "|    reward             | -1.2221292 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 178        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 984       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -191      |\n",
      "|    reward             | -8.653692 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.43e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 984       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.64     |\n",
      "|    explained_variance | 0.001     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 31.8      |\n",
      "|    reward             | 0.6219978 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 31.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 984       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 22.3      |\n",
      "|    reward             | 1.8941143 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 32.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 984       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -15.5     |\n",
      "|    reward             | 1.8065089 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 25.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 985       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 14.9      |\n",
      "|    reward             | -1.246654 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 36.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 985        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 145        |\n",
      "|    reward             | -13.560422 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 774        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 985        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.69      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 163        |\n",
      "|    reward             | -33.962936 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.29e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 986       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.69     |\n",
      "|    explained_variance | -0.000654 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    reward             | 2.4381406 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 33.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 986       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | 0.000203  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -84.2     |\n",
      "|    reward             | -1.179484 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 339       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 986        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.68      |\n",
      "|    explained_variance | -0.000977  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | 0.12509519 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 986        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.66      |\n",
      "|    explained_variance | -2.26e-06  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 31.2       |\n",
      "|    reward             | 10.1643305 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 173        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 986       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | -4.41e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -168      |\n",
      "|    reward             | -6.456506 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 743       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 987      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.67    |\n",
      "|    explained_variance | 5.36e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 262      |\n",
      "|    reward             | 12.5072  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.41e+03 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 987         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.69       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -7.28       |\n",
      "|    reward             | -0.60366046 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.13        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 987       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.65     |\n",
      "|    explained_variance | 9.54e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 20.1      |\n",
      "|    reward             | -1.689988 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 987        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.67      |\n",
      "|    explained_variance | -0.000954  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -27.8      |\n",
      "|    reward             | -2.3020663 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 41.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 987       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | -0.000771 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -82.9     |\n",
      "|    reward             | -0.840895 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 273       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 987        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.66      |\n",
      "|    explained_variance | -3.7e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 139        |\n",
      "|    reward             | -36.068016 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 794        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 987        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 2.26       |\n",
      "|    reward             | 0.84277666 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.723      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 987       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.68     |\n",
      "|    explained_variance | 0.000832  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 33.6      |\n",
      "|    reward             | 14.175156 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 53.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | -0.000982 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 46.7      |\n",
      "|    reward             | 1.127486  |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 47.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 988         |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.67       |\n",
      "|    explained_variance | -0.00181    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | 5.21        |\n",
      "|    reward             | -0.48321527 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 7.95        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 988      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.67    |\n",
      "|    explained_variance | -0.00111 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -23      |\n",
      "|    reward             | 8.334852 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 29.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 988      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 7.06     |\n",
      "|    reward             | 11.39473 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 177      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -18.8     |\n",
      "|    reward             | 1.2797521 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 12.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.65     |\n",
      "|    explained_variance | -0.00432  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -0.391    |\n",
      "|    reward             | 0.5586183 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 5.93      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.65     |\n",
      "|    explained_variance | -0.000239 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 20.3      |\n",
      "|    reward             | 8.787832  |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 988      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.63    |\n",
      "|    explained_variance | -0.00392 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -20.8    |\n",
      "|    reward             | -5.15426 |\n",
      "|    std                | 0.989    |\n",
      "|    value_loss         | 16.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 988       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 53.5      |\n",
      "|    reward             | 3.7040567 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 158       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 989       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.62     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 54.3      |\n",
      "|    reward             | 6.407028  |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 267       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 989        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.6       |\n",
      "|    explained_variance | -0.0274    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -1.34      |\n",
      "|    reward             | -0.6547579 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 0.486      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 989         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.62       |\n",
      "|    explained_variance | -0.0332     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 9.77        |\n",
      "|    reward             | -0.19521104 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 4.86        |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c,\n",
    "                                tb_log_name='a2c',\n",
    "                                total_timesteps=50000) if if_using_a2c else None\n",
    "\n",
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=50000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d13207d-de25-4bbc-840d-019a6b673426",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + '/agent_a2c') if if_using_a2c else None\n",
    "\n",
    "trained_ppo.save(TRAINED_MODEL_DIR + '/agent_ppo') if if_using_ppo else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_rl_env",
   "language": "python",
   "name": "fin_rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
